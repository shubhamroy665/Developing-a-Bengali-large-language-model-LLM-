{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62f8a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------\n",
    "Transformer using pytorch and numpy\n",
    "-----------------------------------------------------------------------------\n",
    "AUTHOR: Soumitra Samanta (soumitra.samanta@gm.rkmvu.ac.in)\n",
    "-----------------------------------------------------------------------------\n",
    "Package required:\n",
    "Numpy: https://numpy.org/\n",
    "Matplotlib: https://matplotlib.org\n",
    "-----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb08bed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset with 1000 lines created at: /home/shubham/Pictures/trans_eng_to_ben/new_dataset_1000_lines_bn.txt\n"
     ]
    }
   ],
   "source": [
    "# Define the input and output file paths\n",
    "input_file = \"/home/shubham/Pictures/trans_eng_to_ben/bn.txt\"\n",
    "output_file = \"/home/shubham/Pictures/trans_eng_to_ben/new_dataset_1000_lines_bn.txt\"\n",
    "\n",
    "# Open the input file and write only the first 1000 lines to the output file\n",
    "with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "    for i, line in enumerate(infile):\n",
    "        if i < 1000:\n",
    "            outfile.write(line)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "print(f\"New dataset with 1000 lines created at: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d4aa92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Self attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dims_embd_ = dims_embd\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor \n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the self attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        y = []\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "class transformer_block_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class transformer_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_encoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer encoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_encoder (int):    Number encoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_endr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_encoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_encoder_ = num_layers_encoder\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class cross_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Cross attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the cross-attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "\n",
    "class transformer_block_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single decoder block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        self.cross_attention_ = cross_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm3_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        y = self.layer_norm1_(y + self.attention_(y))\n",
    "        y = self.droput_ops_(y)\n",
    "        y = self.layer_norm2_(y + self.cross_attention_(x, y))\n",
    "        y = self.droput_ops_(y)\n",
    "        y = self.layer_norm3_(y + self.ffnn_(y))\n",
    "        y = self.droput_ops_(y)\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    \n",
    "class transformer_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_decoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer decoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_decoder (int):    Number decoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_dcdr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_decoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_decoder_ = num_layers_decoder\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        for block in self.trs_dcdr_blocks_:\n",
    "            x = block(x, y)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "       \n",
    "    \n",
    "dims_embd = 10\n",
    "num_data_points = 100\n",
    "batch_size = 5\n",
    "num_hidden_nodes_ffnn = 1024\n",
    "dropout_prob = 0.2\n",
    "num_layers_encoder = 2\n",
    "\n",
    "x = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "y = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "\n",
    "# Test Self-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_self_attention_layer = self_attention_layer(dims_embd)\n",
    "print('Self-attention layer models is: \\n{}' .format(model_self_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_self_attention_layer(x)\n",
    "print('Self-attention layer input size: {}' .format(x.shape))\n",
    "print('Self-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "        \n",
    "# Test Transformer encoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_encoder = transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer block models is: \\n{}' .format(model_transformer_block_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_encoder(x)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer encoder input output size \n",
    "print('='*70)\n",
    "model_transformer_encoder = transformer_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer encoder models is: \\n{}' .format(model_transformer_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer encoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_encoder(x)\n",
    "print('Transformer encoder input size: {}' .format(x.shape))\n",
    "print('Transformer encoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Cross-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_cross_attention_layer = cross_attention_layer(dims_embd)\n",
    "print('Cross-attention layer models is: \\n{}' .format(model_cross_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_cross_attention_layer(x, y)\n",
    "print('Cross-attention layer input size: {}' .format(x.shape))\n",
    "print('Cross-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer decoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_decoder = transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer decoder block models is: \\n{}' .format(model_transformer_block_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_decoder(x, y)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer decoder input output size \n",
    "print('='*70)\n",
    "model_transformer_decoder = transformer_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer decoder models is: \\n{}' .format(model_transformer_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_decoder(x, y)\n",
    "print('Transformer decoder input size: {}' .format(x.shape))\n",
    "print('Transformer decoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a66923df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# File paths\n",
    "ENGLISH_FILE_PATH = '/home/shubham/Pictures/trans_eng_to_ben/en.txt'  # Replace with actual file path\n",
    "BENGALI_FILE_PATH = '/home/shubham/Pictures/trans_eng_to_ben/bn.txt'  # Replace with actual file path\n",
    "\n",
    "# Loading data\n",
    "def load_data(english_file, bengali_file):\n",
    "    with open(english_file, 'r', encoding='utf-8') as f:\n",
    "        english_sentences = f.readlines()\n",
    "    with open(bengali_file, 'r', encoding='utf-8') as f:\n",
    "        bengali_sentences = f.readlines()\n",
    "    \n",
    "    # Clean and strip sentences\n",
    "    english_sentences = [line.strip() for line in english_sentences]\n",
    "    bengali_sentences = [line.strip() for line in bengali_sentences]\n",
    "    \n",
    "    return english_sentences, bengali_sentences\n",
    "\n",
    "# Tokenization and encoding\n",
    "def tokenize_and_pad(sentences, tokenizer, max_len):\n",
    "    tokenized = [tokenizer.encode(sentence) for sentence in sentences]\n",
    "    return [sent + [0] * (max_len - len(sent)) for sent in tokenized]\n",
    "\n",
    "# PyTorch Dataset and Dataloader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, english, bengali, english_tokenizer, bengali_tokenizer, max_len):\n",
    "        self.english = tokenize_and_pad(english, english_tokenizer, max_len)\n",
    "        self.bengali = tokenize_and_pad(bengali, bengali_tokenizer, max_len)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.english)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.english[idx]), torch.tensor(self.bengali[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d8250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487eff0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fe3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torchsummary import summary\n",
    "import math\n",
    "\n",
    "class self_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Self-attention class initialization\n",
    "        \n",
    "        Input:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor \n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the self-attention layer\n",
    "        \n",
    "        Input:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "            - y (torch tensor): Output of self-attention\n",
    "        \"\"\"\n",
    "        \n",
    "        Q = self.W_q_(x)\n",
    "        K = self.W_k_(x)\n",
    "        V = self.W_v_(x)\n",
    "        \n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.dims_embd_)\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        y = attention @ V\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class transformer_block_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single encoder block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Input:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the feed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in linear layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.dropout_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Input:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "            - x (torch tensor): Output of transformer block encoder\n",
    "        \"\"\"\n",
    "        \n",
    "        attn_out = self.attention_(x)\n",
    "        x = self.layer_norm1_(x + attn_out)\n",
    "        x = self.dropout_(x)\n",
    "        \n",
    "        ffnn_out = self.ffnn_(x)\n",
    "        x = self.layer_norm2_(x + ffnn_out)\n",
    "        x = self.dropout_(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class transformer_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_encoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer encoder class initialization\n",
    "        \n",
    "        Input:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the feed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in linear layers\n",
    "            - num_layers_encoder (int):    Number of encoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_encoder)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Input:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "            - x (torch tensor): Output of transformer encoder\n",
    "        \"\"\"\n",
    "        \n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class cross_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross-attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Cross-attention class initialization\n",
    "        \n",
    "        Input:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dims_embd_ = dims_embd  # Define dims_embd_ as an attribute\n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the cross-attention layer\n",
    "        \n",
    "        Input:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "            - y (torch tensor): Output of cross-attention\n",
    "        \"\"\"\n",
    "        \n",
    "        Q = self.W_q_(y)\n",
    "        K = self.W_k_(x)\n",
    "        V = self.W_v_(x)\n",
    "        \n",
    "        # Use self.dims_embd_ in the forward pass\n",
    "        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.dims_embd_)\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        y = attention @ V\n",
    "        \n",
    "        return y\n",
    "\n",
    "    \n",
    "\n",
    "class transformer_block_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single decoder block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Input:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the feed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in linear layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        self.cross_attention_ = cross_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm3_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.dropout_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Input:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "            - y (torch tensor): Output of transformer block decoder\n",
    "        \"\"\"\n",
    "        \n",
    "        y = self.layer_norm1_(y + self.attention_(y))\n",
    "        y = self.dropout_(y)\n",
    "        y = self.layer_norm2_(y + self.cross_attention_(x, y))\n",
    "        y = self.dropout_(y)\n",
    "        y = self.layer_norm3_(y + self.ffnn_(y))\n",
    "        y = self.dropout_(y)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "class transformer_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_decoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer decoder class initialization\n",
    "        \n",
    "        Input:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the feed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in linear layers\n",
    "            - num_layers_decoder (int):    Number of decoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder_blocks = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_decoder)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer decoder\n",
    "        \n",
    "        Input:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "            - y (torch tensor): Output of transformer decoder\n",
    "        \"\"\"\n",
    "        \n",
    "        for block in self.decoder_blocks:\n",
    "            y = block(x, y)\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0ec43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and initialize tokenizers (substitute with actual tokenizer creation)\n",
    "english_sentences, bengali_sentences = load_data(ENGLISH_FILE_PATH, BENGALI_FILE_PATH)\n",
    "english_tokenizer = None  # Implement tokenizer\n",
    "bengali_tokenizer = None  # Implement tokenizer\n",
    "\n",
    "# Dataset and Dataloader\n",
    "train_dataset = TranslationDataset(english_sentences, bengali_sentences, english_tokenizer, bengali_tokenizer, max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model components\n",
    "encoder = transformer_encoder(embedding_dim, ff_dim, dropout_prob, num_layers)\n",
    "decoder = transformer_decoder(embedding_dim, ff_dim, dropout_prob, num_layers)\n",
    "model = Transformer(encoder, decoder, vocab_size_tgt=bengali_tokenizer.vocab_size)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for src, tgt in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Translation function\n",
    "def translate(model, sentence, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode source sentence\n",
    "        src = torch.tensor(tokenize_and_pad([sentence], english_tokenizer, max_len)).to(model.device)\n",
    "        tgt = torch.tensor([[bengali_tokenizer.vocab_size]])  # Initialize with start token\n",
    "        for i in range(max_len):\n",
    "            output = model(src, tgt)\n",
    "            pred_token = output.argmax(dim=-1)[:, -1]\n",
    "            tgt = torch.cat((tgt, pred_token.unsqueeze(0)), dim=-1)\n",
    "            if pred_token.item() == 0:  # Stop on <eos>\n",
    "                break\n",
    "        return bengali_tokenizer.decode(tgt.squeeze().tolist())\n",
    "\n",
    "# Test Translation\n",
    "test_sentence = \"Hello, how are you?\"\n",
    "translated_sentence = translate(model, test_sentence)\n",
    "print(f\"English: {test_sentence}\")\n",
    "print(f\"Bengali: {translated_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bcdc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Self-attention layer models is: \n",
      "self_attention_layer(\n",
      "  (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Self-attention layer input size: torch.Size([5, 100, 10])\n",
      "Self-attention layer output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer block models is: \n",
      "transformer_block_encoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      "  (dropout_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "         LayerNorm-5              [-1, 100, 10]              20\n",
      "           Dropout-6              [-1, 100, 10]               0\n",
      "            Linear-7            [-1, 100, 1024]          11,264\n",
      "              ReLU-8            [-1, 100, 1024]               0\n",
      "            Linear-9              [-1, 100, 10]          10,250\n",
      "        LayerNorm-10              [-1, 100, 10]              20\n",
      "          Dropout-11              [-1, 100, 10]               0\n",
      "================================================================\n",
      "Total params: 21,884\n",
      "Trainable params: 21,884\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.63\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 1.72\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 100, 10])\n",
      "Transformer block output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer encoder models is: \n",
      "transformer_encoder(\n",
      "  (encoder_blocks): ModuleList(\n",
      "    (0-1): 2 x transformer_block_encoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      )\n",
      "      (dropout_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "         LayerNorm-5              [-1, 100, 10]              20\n",
      "           Dropout-6              [-1, 100, 10]               0\n",
      "            Linear-7            [-1, 100, 1024]          11,264\n",
      "              ReLU-8            [-1, 100, 1024]               0\n",
      "            Linear-9              [-1, 100, 10]          10,250\n",
      "        LayerNorm-10              [-1, 100, 10]              20\n",
      "          Dropout-11              [-1, 100, 10]               0\n",
      "transformer_block_encoder-12              [-1, 100, 10]               0\n",
      "           Linear-13              [-1, 100, 10]             110\n",
      "           Linear-14              [-1, 100, 10]             110\n",
      "           Linear-15              [-1, 100, 10]             110\n",
      "self_attention_layer-16              [-1, 100, 10]               0\n",
      "        LayerNorm-17              [-1, 100, 10]              20\n",
      "          Dropout-18              [-1, 100, 10]               0\n",
      "           Linear-19            [-1, 100, 1024]          11,264\n",
      "             ReLU-20            [-1, 100, 1024]               0\n",
      "           Linear-21              [-1, 100, 10]          10,250\n",
      "        LayerNorm-22              [-1, 100, 10]              20\n",
      "          Dropout-23              [-1, 100, 10]               0\n",
      "transformer_block_encoder-24              [-1, 100, 10]               0\n",
      "================================================================\n",
      "Total params: 43,768\n",
      "Trainable params: 43,768\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.28\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 3.45\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder input size: torch.Size([5, 100, 10])\n",
      "Transformer encoder output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Cross-attention layer models is: \n",
      "cross_attention_layer(\n",
      "  (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2663/1461004299.py\", line 58, in <module>\n",
      "    y_bar = model_cross_attention_layer(x, y)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2663/1716964706.py\", line 200, in forward\n",
      "    scores = Q @ K.transpose(-2, -1) / math.sqrt(self.dims_embd_)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1931, in __getattr__\n",
      "    raise AttributeError(\n",
      "AttributeError: 'cross_attention_layer' object has no attribute 'dims_embd_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2168, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1110, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 992, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 804, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/stack_data/core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/stack_data/core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/stack_data/core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/executing/executing.py\", line 332, in asttext\n",
      "    self._asttext = ASTText(self.text, tree=self.tree, filename=self.filename)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/asttokens/asttokens.py\", line 305, in __init__\n",
      "    super(ASTText, self).__init__(source_text, filename)\n",
      "  File \"/home/shubham/.local/lib/python3.10/site-packages/asttokens/asttokens.py\", line 47, in __init__\n",
      "    source_text = six.ensure_text(source_text)\n",
      "AttributeError: module 'six' has no attribute 'ensure_text'\n"
     ]
    }
   ],
   "source": [
    "dims_embd = 10\n",
    "num_data_points = 100\n",
    "batch_size = 5\n",
    "num_hidden_nodes_ffnn = 1024\n",
    "dropout_prob = 0.2\n",
    "num_layers_encoder = 2\n",
    "\n",
    "x = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "y = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "\n",
    "# Test Self-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_self_attention_layer = self_attention_layer(dims_embd)\n",
    "print('Self-attention layer models is: \\n{}' .format(model_self_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_self_attention_layer(x)\n",
    "print('Self-attention layer input size: {}' .format(x.shape))\n",
    "print('Self-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "        \n",
    "# Test Transformer encoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_encoder = transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer block models is: \\n{}' .format(model_transformer_block_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_encoder(x)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer encoder input output size \n",
    "print('='*70)\n",
    "model_transformer_encoder = transformer_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer encoder models is: \\n{}' .format(model_transformer_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer encoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_encoder(x)\n",
    "print('Transformer encoder input size: {}' .format(x.shape))\n",
    "print('Transformer encoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Cross-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_cross_attention_layer = cross_attention_layer(dims_embd)\n",
    "print('Cross-attention layer models is: \\n{}' .format(model_cross_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_cross_attention_layer(x, y)\n",
    "print('Cross-attention layer input size: {}' .format(x.shape))\n",
    "print('Cross-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer decoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_decoder = transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer decoder block models is: \\n{}' .format(model_transformer_block_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_decoder(x, y)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer decoder input output size \n",
    "print('='*70)\n",
    "model_transformer_decoder = transformer_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer decoder models is: \\n{}' .format(model_transformer_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_decoder(x, y)\n",
    "print('Transformer decoder input size: {}' .format(x.shape))\n",
    "print('Transformer decoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Set the file paths for English and Bengali datasets\n",
    "ENGLISH_FILE_PATH = '/home/shubham/Pictures/trans_eng_to_ben/en.txt'  # replace with your file path\n",
    "BENGALI_FILE_PATH = '/home/shubham/Pictures/trans_eng_to_ben/bn.txt'  # replace with your file path\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data(english_file, bengali_file):\n",
    "    with open(english_file, 'r', encoding='utf-8') as f:\n",
    "        english_sentences = f.readlines()\n",
    "    with open(bengali_file, 'r', encoding='utf-8') as f:\n",
    "        bengali_sentences = f.readlines()\n",
    "    \n",
    "    # Clean and trim whitespaces\n",
    "    english_sentences = [line.strip() for line in english_sentences]\n",
    "    bengali_sentences = [line.strip() for line in bengali_sentences]\n",
    "    \n",
    "    return english_sentences, bengali_sentences\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(lang_sentences):\n",
    "    tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "        lang_sentences, target_vocab_size=2**13)\n",
    "    return tokenizer\n",
    "\n",
    "# Prepare tokenized datasets with padding\n",
    "def encode(lang1, lang2, tokenizer1, tokenizer2):\n",
    "    lang1_encoded = [tokenizer1.encode(sentence) for sentence in lang1]\n",
    "    lang2_encoded = [tokenizer2.encode(sentence) for sentence in lang2]\n",
    "    \n",
    "    max_len = max(max(len(seq) for seq in lang1_encoded), max(len(seq) for seq in lang2_encoded))\n",
    "    lang1_padded = tf.keras.preprocessing.sequence.pad_sequences(lang1_encoded, maxlen=max_len, padding='post')\n",
    "    lang2_padded = tf.keras.preprocessing.sequence.pad_sequences(lang2_encoded, maxlen=max_len, padding='post')\n",
    "    \n",
    "    return lang1_padded, lang2_padded\n",
    "\n",
    "# Load data and initialize tokenizers\n",
    "english_sentences, bengali_sentences = load_data(ENGLISH_FILE_PATH, BENGALI_FILE_PATH)\n",
    "english_tokenizer = tokenize(english_sentences)\n",
    "bengali_tokenizer = tokenize(bengali_sentences)\n",
    "\n",
    "# Encode and pad the sentences\n",
    "english_padded, bengali_padded = encode(english_sentences, bengali_sentences, english_tokenizer, bengali_tokenizer)\n",
    "\n",
    "# Transformer Architecture\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, vocab_size_src, vocab_size_tgt, embedding_dim=256, num_heads=8, ff_dim=512, num_layers=4):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = [\n",
    "            tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.decoder = [\n",
    "            tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size_tgt)\n",
    "        \n",
    "    def call(self, src, tgt, training):\n",
    "        x = src\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x, x)\n",
    "        \n",
    "        y = tgt\n",
    "        for layer in self.decoder:\n",
    "            y = layer(y, x)\n",
    "        \n",
    "        return self.dense(y)\n",
    "\n",
    "# Model initialization\n",
    "embedding_dim = 256\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size_src=english_tokenizer.vocab_size, \n",
    "    vocab_size_tgt=bengali_tokenizer.vocab_size,\n",
    "    embedding_dim=embedding_dim, num_heads=num_heads, ff_dim=ff_dim, num_layers=num_layers\n",
    ")\n",
    "\n",
    "# Prepare dataset for training\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((english_padded, bengali_padded))\n",
    "dataset = dataset.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "EPOCHS = 10\n",
    "model.fit(dataset, epochs=EPOCHS)\n",
    "\n",
    "# Translate a given English sentence to Bengali\n",
    "def translate(sentence):\n",
    "    # Preprocess the input\n",
    "    sentence = english_tokenizer.encode(sentence)\n",
    "    sentence = tf.keras.preprocessing.sequence.pad_sequences([sentence], maxlen=english_padded.shape[1], padding='post')\n",
    "    \n",
    "    # Predict using the model\n",
    "    prediction = model(sentence, training=False)\n",
    "    predicted_sentence = [np.argmax(p) for p in prediction[0]]\n",
    "    \n",
    "    # Decode the predicted sentence\n",
    "    translated_sentence = bengali_tokenizer.decode(predicted_sentence)\n",
    "    return translated_sentence\n",
    "\n",
    "# Example translation\n",
    "english_text = \"Hello, how are you?\"\n",
    "bengali_translation = translate(english_text)\n",
    "print(f\"English: {english_text}\")\n",
    "print(f\"Bengali Translation: {bengali_translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf9554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define self-attention, transformer encoder, and decoder as provided earlier\n",
    "# Copy the `self_attention_layer`, `transformer_block_encoder`, `transformer_encoder`, \n",
    "# `cross_attention_layer`, `transformer_block_decoder`, `transformer_decoder` classes here\n",
    "\n",
    "# Set dimensions\n",
    "dims_embd = 128  # You might need to adjust this based on vocabulary size and embedding size\n",
    "num_hidden_nodes_ffnn = 512\n",
    "dropout_prob = 0.1\n",
    "num_layers_encoder = 2\n",
    "num_layers_decoder = 2\n",
    "\n",
    "# Model instantiation\n",
    "encoder = transformer_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "decoder = transformer_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_decoder)\n",
    "\n",
    "class TransformerTranslationModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size_src, vocab_size_tgt, dims_embd):\n",
    "        super(TransformerTranslationModel, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embedding = nn.Embedding(vocab_size_src, dims_embd)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size_tgt, dims_embd)\n",
    "        self.fc_out = nn.Linear(dims_embd, vocab_size_tgt)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.src_embedding(src)\n",
    "        tgt_embedded = self.tgt_embedding(tgt)\n",
    "        encoded = self.encoder(src_embedded)\n",
    "        decoded = self.decoder(encoded, tgt_embedded)\n",
    "        output = self.fc_out(decoded)\n",
    "        return output\n",
    "\n",
    "# Dataset class to load and process text\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, tgt_texts, src_vocab, tgt_vocab, max_len=100):\n",
    "        self.src_texts = src_texts\n",
    "        self.tgt_texts = tgt_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.src_texts[idx]\n",
    "        tgt_text = self.tgt_texts[idx]\n",
    "        \n",
    "        src_indices = [self.src_vocab.get(word, self.src_vocab['<UNK>']) for word in src_text.split()[:self.max_len]]\n",
    "        tgt_indices = [self.tgt_vocab.get(word, self.tgt_vocab['<UNK>']) for word in tgt_text.split()[:self.max_len]]\n",
    "        \n",
    "        return torch.tensor(src_indices), torch.tensor(tgt_indices)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "def load_dataset(file_path, num_lines=1000):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[:num_lines]\n",
    "    eng_texts = [line.split('\\t')[0].strip() for line in lines]\n",
    "    ben_texts = [line.split('\\t')[1].strip() for line in lines]\n",
    "    return eng_texts, ben_texts\n",
    "\n",
    "# Create a vocabulary dictionary for both languages\n",
    "def build_vocab(texts, max_size=5000):\n",
    "    from collections import Counter\n",
    "    word_counts = Counter(word for text in texts for word in text.split())\n",
    "    vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common(max_size), start=2)}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "# Load the English and Bengali data\n",
    "eng_texts = load_dataset('/home/shubham/Pictures/trans_eng_to_ben/en.txt', num_lines=1000)\n",
    "ben_texts=load_dataset('/home/shubham/Pictures/trans_eng_to_ben/bn.txt', num_lines=1000)\n",
    "\n",
    "# Build vocabularies\n",
    "src_vocab = build_vocab(eng_texts)\n",
    "tgt_vocab = build_vocab(ben_texts)\n",
    "\n",
    "# Train-test split\n",
    "train_eng, val_eng, train_ben, val_ben = train_test_split(eng_texts, ben_texts, test_size=0.2)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "train_dataset = TranslationDataset(train_eng, train_ben, src_vocab, tgt_vocab)\n",
    "val_dataset = TranslationDataset(val_eng, val_ben, src_vocab, tgt_vocab)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "# Initialize the model\n",
    "vocab_size_src = len(src_vocab)\n",
    "vocab_size_tgt = len(tgt_vocab)\n",
    "model = TransformerTranslationModel(encoder, decoder, vocab_size_src, vocab_size_tgt, dims_embd)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab['<PAD>'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output = output.view(-1, vocab_size_tgt)\n",
    "        tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for src, tgt in val_loader:\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            output = output.view(-1, vocab_size_tgt)\n",
    "            tgt = tgt[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output, tgt)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f3b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191cb714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7ed170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c48f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42288f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
